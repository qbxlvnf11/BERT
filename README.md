
It is posted by Yong (예비개발자).

Blog URL: https://blog.naver.com/qbxlvnf11

LinkedIn: https://www.linkedin.com/in/taeyong-kong-016bb2154


- Case of contextualized embedding model BERT (Bidirectional Encoder Representations from Transformers) utilization
- BERT fine-tune etc.
- Upload code as a Jupiter Notebook file (.ipynb) for immediate understanding


Contents
=============

- BERT implemented by pytorch for multi class classification
- BERT implemented by ktrain in keras for binary classification

Datasets
=============

- News Aggregater

https://www.kaggle.com/uciml/news-aggregator-dataset

- Detecting Insults in Social Commentary

https://www.kaggle.com/c/detecting-insults-in-social-commentary

References
=============

- Paper [ BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding ]

https://arxiv.org/abs/1810.04805

- Keras ktrain

https://towardsdatascience.com/bert-text-classification-in-3-lines-of-code-using-keras-264db7e7a358

- Code

https://github.com/shudima/notebooks

https://github.com/amaiya/ktrain/blob/master/examples/text/IMDb-BERT.ipynb

Code correction
=============

- BERT_base_uncased_multi_class_classification.ipynb

[66] block: train_loss -> loss

- BERT implemented by ktrain in keras for binary classification

[4] block: changing name of X axis & Y asxis
